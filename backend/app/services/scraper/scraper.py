#!/usr/bin/env python3
"""
Gæ¤œå®šå¯¾ç­–ãƒ„ãƒ¼ãƒ« - Webã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°åŸºç›¤
Playwright ã‚’ç”¨ã„ã¦æŒ‡å®š URL ã® HTML ã‚’å–å¾—ã—ã€robots.txt ã‚’éµå®ˆã™ã‚‹åŸºç›¤
"""

import asyncio
import sys
import time
import re
from pathlib import Path
from urllib.parse import urljoin, urlparse
from urllib.robotparser import RobotFileParser
from datetime import datetime
import requests
from playwright.async_api import async_playwright


class GExamScraper:
    """Gæ¤œå®šå¯¾ç­–ç”¨ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼"""
    
    def __init__(self):
        self.user_agent = "Mozilla/5.0 (compatible; GExamBot/1.0)"
        self.min_delay = 1.0  # æœ€å°ã‚¦ã‚§ã‚¤ãƒˆæ™‚é–“ï¼ˆç§’ï¼‰
        self.output_dir = Path("./html")
        self.last_request_time = 0
    
    def check_robots_txt(self, url: str) -> bool:
        """
        robots.txt ã‚’ç¢ºèªã—ã¦ã‚¯ãƒ­ãƒ¼ãƒ«å¯èƒ½ã‹ãƒã‚§ãƒƒã‚¯
        
        Args:
            url: ãƒã‚§ãƒƒã‚¯å¯¾è±¡ã®URL
            
        Returns:
            bool: ã‚¯ãƒ­ãƒ¼ãƒ«å¯èƒ½ãªå ´åˆ True
        """
        try:
            parsed_url = urlparse(url)
            base_url = f"{parsed_url.scheme}://{parsed_url.netloc}"
            robots_url = urljoin(base_url, "/robots.txt")
            
            print(f"ğŸ“‹ robots.txt ã‚’ãƒã‚§ãƒƒã‚¯ä¸­: {robots_url}")
            
            rp = RobotFileParser()
            rp.set_url(robots_url)
            rp.read()
            
            can_fetch = rp.can_fetch(self.user_agent, url)
            
            if can_fetch:
                print(f"âœ… robots.txt: ã‚¯ãƒ­ãƒ¼ãƒ«è¨±å¯")
            else:
                print(f"âŒ robots.txt: ã‚¯ãƒ­ãƒ¼ãƒ«ç¦æ­¢")
                
            return can_fetch
            
        except Exception as e:
            print(f"âš ï¸  robots.txt ã®ç¢ºèªã§ã‚¨ãƒ©ãƒ¼: {e}")
            print("ğŸ¤” å®‰å…¨ã®ãŸã‚ç¶šè¡Œã‚’ä¸­æ­¢ã—ã¾ã™")
            return False
    
    def generate_filename(self, url: str) -> tuple[str, str]:
        """
        ä¿å­˜ç”¨ã®ãƒ•ã‚¡ã‚¤ãƒ«åã¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ‘ã‚¹ã‚’ç”Ÿæˆ
        
        Args:
            url: å¯¾è±¡URL
            
        Returns:
            tuple: (ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ‘ã‚¹, ãƒ•ã‚¡ã‚¤ãƒ«å)
        """
        parsed_url = urlparse(url)
        domain = parsed_url.netloc
        
        # URLã‹ã‚‰ã‚¹ãƒ©ã‚°ã‚’ç”Ÿæˆï¼ˆãƒ•ã‚¡ã‚¤ãƒ«åã«ä½¿ç”¨å¯èƒ½ãªæ–‡å­—ã®ã¿ï¼‰
        path = parsed_url.path.strip("/")
        if path:
            slug = re.sub(r'[^\w\-_.]', '_', path)
            # é•·ã™ãã‚‹å ´åˆã¯å…ˆé ­50æ–‡å­—ã«åˆ¶é™
            slug = slug[:50] if len(slug) > 50 else slug
        else:
            slug = "index"
        
        # æ—¥ä»˜ã‚’è¿½åŠ 
        date_str = datetime.now().strftime("%Y-%m-%d")
        filename = f"{date_str}_{slug}.html"
        
        directory = self.output_dir / domain
        
        return str(directory), filename
    
    def apply_rate_limit(self):
        """ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚’é©ç”¨ï¼ˆ1ç§’ä»¥ä¸Šã®ã‚¦ã‚§ã‚¤ãƒˆï¼‰"""
        current_time = time.time()
        elapsed = current_time - self.last_request_time
        
        if elapsed < self.min_delay:
            wait_time = self.min_delay - elapsed
            print(f"â±ï¸  ãƒ¬ãƒ¼ãƒˆåˆ¶é™: {wait_time:.2f}ç§’å¾…æ©Ÿä¸­...")
            time.sleep(wait_time)
        
        self.last_request_time = time.time()
    
    async def fetch_html(self, url: str) -> str:
        """
        Playwright ã‚’ä½¿ç”¨ã—ã¦HTMLã‚’å–å¾—
        
        Args:
            url: å–å¾—å¯¾è±¡ã®URL
            
        Returns:
            str: å–å¾—ã—ãŸHTML
        """
        print(f"ğŸŒ HTMLã‚’å–å¾—ä¸­: {url}")
        
        async with async_playwright() as p:
            # ãƒ˜ãƒƒãƒ‰ãƒ¬ã‚¹Chrome ã‚’èµ·å‹•
            browser = await p.chromium.launch(headless=True)
            
            # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’è¨­å®š
            context = await browser.new_context(
                user_agent=self.user_agent,
                viewport={'width': 1920, 'height': 1080}
            )
            
            page = await context.new_page()
            
            try:
                # ãƒšãƒ¼ã‚¸ã‚’èª­ã¿è¾¼ã¿ï¼ˆSPAå¯¾å¿œã®ãŸã‚ååˆ†ãªå¾…æ©Ÿæ™‚é–“ï¼‰
                await page.goto(url, wait_until='networkidle', timeout=30000)
                
                # è¿½åŠ ã®å¾…æ©Ÿæ™‚é–“ï¼ˆJavaScriptå®Ÿè¡Œå®Œäº†ã‚’ç¢ºä¿ï¼‰
                await page.wait_for_timeout(2000)
                
                # HTMLå–å¾—
                html_content = await page.content()
                
                print(f"âœ… HTMLå–å¾—å®Œäº† ({len(html_content):,} æ–‡å­—)")
                return html_content
                
            except Exception as e:
                print(f"âŒ HTMLå–å¾—å¤±æ•—: {e}")
                raise
            finally:
                await browser.close()
    
    def save_html(self, html_content: str, directory: str, filename: str) -> str:
        """
        HTMLã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        
        Args:
            html_content: ä¿å­˜ã™ã‚‹HTML
            directory: ä¿å­˜å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
            filename: ãƒ•ã‚¡ã‚¤ãƒ«å
            
        Returns:
            str: ä¿å­˜ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        """
        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        dir_path = Path(directory)
        dir_path.mkdir(parents=True, exist_ok=True)
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        file_path = dir_path / filename
        
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(html_content)
        
        print(f"ğŸ’¾ ä¿å­˜å®Œäº†: {file_path}")
        return str(file_path)
    
    async def scrape(self, url: str) -> str:
        """
        ãƒ¡ã‚¤ãƒ³ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å‡¦ç†
        
        Args:
            url: ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å¯¾è±¡ã®URL
            
        Returns:
            str: ä¿å­˜ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        """
        print(f"ğŸš€ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹: {url}")
        
        # 1. robots.txt ãƒã‚§ãƒƒã‚¯
        if not self.check_robots_txt(url):
            raise ValueError("robots.txt ã«ã‚ˆã‚Šã‚¯ãƒ­ãƒ¼ãƒ«ãŒç¦æ­¢ã•ã‚Œã¦ã„ã¾ã™")
        
        # 2. ãƒ¬ãƒ¼ãƒˆåˆ¶é™é©ç”¨
        self.apply_rate_limit()
        
        # 3. HTMLå–å¾—
        html_content = await self.fetch_html(url)
        
        # 4. ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        directory, filename = self.generate_filename(url)
        file_path = self.save_html(html_content, directory, filename)
        
        print(f"ğŸ‰ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Œäº†!")
        return file_path


async def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    if len(sys.argv) != 2:
        print("ä½¿ç”¨æ–¹æ³•: python scraper.py <target_url>")
        print("ä¾‹: python scraper.py https://example.com/g-exam-article")
        sys.exit(1)
    
    target_url = sys.argv[1]
    
    # URLå½¢å¼ã®ç°¡å˜ãªãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
    if not target_url.startswith(('http://', 'https://')):
        print("âŒ ã‚¨ãƒ©ãƒ¼: URLã¯ http:// ã¾ãŸã¯ https:// ã§å§‹ã¾ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™")
        sys.exit(1)
    
    scraper = GExamScraper()
    
    try:
        file_path = await scraper.scrape(target_url)
        print(f"\nğŸ“ ä¿å­˜å…ˆ: {file_path}")
        
    except KeyboardInterrupt:
        print("\nâš ï¸  ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã‚ˆã£ã¦ä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())